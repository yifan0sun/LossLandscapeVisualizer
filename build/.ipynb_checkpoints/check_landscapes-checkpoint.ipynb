{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e33a9cb-a74b-4260-be5f-64abd9b2d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "from visualize_landscape import *\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_num_threads(3)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "daba1293-3d34-4fa5-b0ee-7f40ec966e22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1\n",
      "5 2\n",
      "5 4\n",
      "5 8\n",
      "5 16\n",
      "10 1\n",
      "10 2\n",
      "10 4\n",
      "10 8\n",
      "10 16\n",
      "25 1\n",
      "25 2\n",
      "25 4\n",
      "25 8\n",
      "25 16\n",
      "50 1\n",
      "50 2\n",
      "50 4\n",
      "50 8\n",
      "50 16\n",
      "100 1\n",
      "100 2\n",
      "100 4\n",
      "100 8\n",
      "100 16\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sadf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8648/1061412565.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0msadf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sadf' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "num_epochs = 1001\n",
    "datasets = [\"two_blobs_separate\", \"two_blobs_overlap\", \"half_moons\", \"spirals\", \"four_corners\"]\n",
    "gridsize = 10\n",
    "\n",
    "\n",
    "\n",
    "for width in [5,10,25,50,100]:\n",
    "    for depth in [1,2,4,8,16]:\n",
    "        can_copy = True\n",
    "                \n",
    "        for dataset in datasets:\n",
    "            for ab_range in [0.1, .2,.5, 1,2,5 ,10]:\n",
    "\n",
    "                for epoch in range(0,num_epochs,10):\n",
    "                    arch = [width for i in range(depth)]\n",
    "\n",
    "\n",
    "                    CACHE_ROOT = \"../backend/landscapes\"\n",
    "\n",
    "                    train_str = 'train' \n",
    "                    cache_dir = os.path.join(CACHE_ROOT, arch_to_name(arch), dataset, train_str, f\"range{ab_range}\",  f\"ep{epoch}\")\n",
    "                    cache_xy_dir = os.path.join(CACHE_ROOT, arch_to_name(arch))\n",
    "\n",
    "                    loss_file = os.path.join(cache_dir, \"loss.npy\")\n",
    "                    a_file = os.path.join(cache_dir, \"a_vals.npy\")\n",
    "                    b_file = os.path.join(cache_dir, \"b_vals.npy\")\n",
    "\n",
    "                    if os.path.exists(loss_file) and os.path.exists(a_file) and os.path.exists(b_file):\n",
    "\n",
    "                        a = np.load(a_file)\n",
    "                        b = np.load(b_file)\n",
    "                        losses = np.load(loss_file)\n",
    "                        \n",
    "                        if losses.shape[0]< gridsize:\n",
    "                            if epoch > 0: print(f\"low grid size  {cache_dir}\")\n",
    "                            can_copy = False\n",
    "                            break\n",
    "                    else: \n",
    "                        print(f\"missing  {cache_dir}\")\n",
    "                        can_copy = False\n",
    "                        break \n",
    "                if not can_copy: break\n",
    "            if not can_copy: break\n",
    "        if can_copy:\n",
    "            print(width,depth)\n",
    "\n",
    "            \n",
    "            \n",
    "sadf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48a875-b718-4efc-b569-3106d0493347",
   "metadata": {},
   "outputs": [],
   "source": [
    "adsf\n",
    "    \n",
    "\n",
    "num_epochs = 1001\n",
    "datasets = [\"two_blobs_separate\", \"two_blobs_overlap\", \"half_moons\", \"spirals\", \"four_corners\"]\n",
    "gridsize = 100\n",
    "\n",
    "\n",
    "CACHE_ROOT = \"../backend/landscapes\"\n",
    "MODELS_DIR = \"../backend/models\"\n",
    "\n",
    "train_str = 'train'  \n",
    " \n",
    "\n",
    "for width in [5,10,25,50,100]:\n",
    "    for depth in [1,2,4,8,16]:\n",
    "        can_copy = True\n",
    "                \n",
    "        for dataset in datasets:\n",
    "            for ab_range in [0.1, .2,.5, 1,2,5 ,10]:\n",
    " \n",
    "                arch = [width for i in range(depth)]\n",
    "\n",
    "\n",
    "                cache_base_dir = os.path.join(CACHE_ROOT, arch_to_name(arch), dataset, train_str, f\"range{ab_range}\")\n",
    "                zrange_file = os.path.join(cache_base_dir, f\"zrange.npy\")\n",
    "\n",
    "\n",
    "                # --- If zrange already exists, skip ---\n",
    "                if not os.path.exists(zrange_file) :\n",
    "\n",
    "                    can_copy = False\n",
    "                    break \n",
    "            if not can_copy: break\n",
    "        if can_copy:\n",
    "            print(width,depth)\n",
    "\n",
    "            \n",
    "            \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef547960-2c09-4397-b330-bac81d136251",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf\n",
    "\n",
    "\n",
    "num_epochs = 1001\n",
    "datasets = [\"two_blobs_separate\", \"two_blobs_overlap\", \"half_moons\", \"spirals\", \"four_corners\"]\n",
    "widths = [5,10,25,50,100] \n",
    "depths = [1,2,4,8,16]  \n",
    "widths = [5] \n",
    "depths = [4,8,16]  \n",
    "ab_ranges = [0.1,.2,.5, 1, 2,5,10]\n",
    "epochs = list(range(0, num_epochs, 10))\n",
    "gridsize = 100\n",
    "\n",
    "for width in widths:\n",
    "    for depth in depths:\n",
    "        arch = [width] * depth\n",
    "        \n",
    "        for dataset in datasets:\n",
    "\n",
    "            for ab_range in ab_ranges:\n",
    "                print(f\"{width=} {depth=} {dataset=} {ab_range=}\", end='')\n",
    "                for epoch in epochs:\n",
    "                    print('.',end='')\n",
    "                    check_if_defective( arch,  dataset, train=True, epoch=epoch, ab_range=ab_range, gridsize=gridsize, remove = True)\n",
    "                print('\\n')\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ff7ceea-7d3d-4751-8ad0-007d01d28389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89882it [00:11, 7964.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss matrix shape counts:\n",
      "1,100: 6652\n",
      "1,50: 1954\n",
      "1,10: 7213\n",
      "1,25: 1954\n",
      "2,10: 8590\n",
      "2,50: 3894\n",
      "2,100: 4066\n",
      "2,25: 1125\n",
      "4,25: 1184\n",
      "4,50: 4548\n",
      "4,100: 3589\n",
      "4,10: 8089\n",
      "4,15: 265\n",
      "8,25: 1621\n",
      "8,50: 4091\n",
      "8,100: 3251\n",
      "8,10: 8712\n",
      "16,25: 1371\n",
      "16,100: 4186\n",
      "16,50: 3778\n",
      "16,10: 8340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def safe_load_npy(file_path, max_retries=5, delay=1.0):\n",
    "    \"\"\"Try loading a .npy file with retries if access fails.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return np.load(file_path)\n",
    "        except (OSError, PermissionError) as e:\n",
    "            print(f\"Access error on {file_path} (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"Non-access error on {file_path}: {type(e).__name__}: {e}\")\n",
    "            break\n",
    "    return None  # if all retries failed or non-retryable error occurred\n",
    "\n",
    "\n",
    "def count_loss_shapes(root_dir):\n",
    "    shape_counter = {}\n",
    "    \n",
    "    for depth in [1,2,4,8,16]:\n",
    "        shape_counter[depth] = {}\n",
    "     \n",
    "\n",
    "    for dirpath, _, filenames in tqdm(os.walk(root_dir)):\n",
    "         \n",
    "        \n",
    "        words = dirpath.split('/')\n",
    "\n",
    "        words = [w for w in words if 'mlp' in w]\n",
    "        if len(words) == 0: continue\n",
    "\n",
    "\n",
    "        arch = [int(w) for w in words[0].split('_')[1:]]\n",
    "        \n",
    "        archstring = '_'.join([str(w) for w in arch])\n",
    "        \n",
    "        archstring = len(arch)\n",
    "      \n",
    "        if archstring not in shape_counter:\n",
    "            shape_counter[archstring] = {}\n",
    "        \n",
    "        for filename in filenames:\n",
    "            if filename == \"loss.npy\":\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    \n",
    "                    data = safe_load_npy(file_path)\n",
    "                    if data is None:\n",
    "                        data = np.load(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "                    asdf\n",
    "                shape = data.shape[0]\n",
    "                if shape not in shape_counter[archstring]:\n",
    "                    shape_counter[archstring][shape] = 1\n",
    "                else:\n",
    "                    shape_counter[archstring][shape] += 1\n",
    "\n",
    "    return shape_counter\n",
    "\n",
    "root_directory = \"../backend/landscapes\" \n",
    "counts = count_loss_shapes(root_directory)\n",
    "\n",
    "print(\"Loss matrix shape counts:\")\n",
    "for arch, countdict in counts.items():\n",
    "    for shape, count in countdict.items():\n",
    "        print(f\"{arch},{shape}: {count}\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    labels = []\n",
    "    values = []\n",
    "\n",
    "    for arch, shape_counts in counts.items():\n",
    "        for shape, count in shape_counts.items():\n",
    "            label = f\"{arch}\\n{shape[0]}x{shape[1]}\"\n",
    "            labels.append(label)\n",
    "            values.append(count)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(labels, values)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Loss Matrix Shape Counts per Architecture\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df88bff-b4fd-482c-b91e-dd344e309c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
